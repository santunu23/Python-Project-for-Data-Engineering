{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08372541",
   "metadata": {},
   "source": [
    "Importing Libraries and setting paths\n",
    "The required files are now available in the project folder.\n",
    "\n",
    "In this lab, you will extract data from CSV, JSON, and XML formats. First, you need to import the appropriate Python libraries to use the relevant functions.\n",
    "\n",
    "The xml library can be used to parse the information from an .xml file format. The .csv and .json file formats can be read using the pandas library. You will use the pandas library to create a data frame format that will store the extracted data from any file.\n",
    "\n",
    "To call the correct function for data extraction, you need to access the file format information. For this access, you can use the glob library.\n",
    "\n",
    "To log the information correctly, you need the date and time information at the point of logging. For this information, you require the datetime package.\n",
    "\n",
    "While glob, xml, and datetime are inbuilt features of Python, you need to install the pandas library to your IDE.\n",
    "\n",
    "Run the following command in a terminal shell to install pandas for python3.11.\n",
    "\n",
    "python3.11 -m pip install pandas\n",
    "\n",
    "Executed!\n",
    "After the installation is complete, you can import all the libraries in etl_code.py using the following commands.\n",
    "\n",
    "import glob \n",
    "import pandas as pd \n",
    "import xml.etree.ElementTree as ET \n",
    "from datetime import datetime \n",
    "Note that you import only the ElementTree function from the xml.etree library because you require that function to parse the data from an XML file format.\n",
    "\n",
    "You also require two file paths that will be available globally in the code for all functions. These are transformed_data.csv, to store the final output data that you can load to a database, and log_file.txt, that stores all the logs.\n",
    "\n",
    "Introduce these paths in the code by adding the following statements:\n",
    "\n",
    "log_file = \"log_file.txt\" \n",
    "target_file = \"transformed_data.csv\" \n",
    "\n",
    "Remember to save your file! You may use Ctrl+S to save the file or click Save in the File tab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248b6a30",
   "metadata": {},
   "source": [
    "#Task 1: Extraction\n",
    "Next, you will develop the functions to extract the data from different file formats. As there will be different functions for the file formats, you'll have to write one function each for the .csv, .json, and the .xml filetypes.\n",
    "\n",
    "You can name these three functions as extract_from_csv(), extract_from_json(), and extract_from_xml(). You need to pass the data file as an argument, file_to_process, to each function.\n",
    "\n",
    "To extract from a CSV file, you can define the function extract_from_csv()as follows using the pandas function read_csv:\n",
    "\n",
    "def extract_from_csv(file_to_process): \n",
    "    dataframe = pd.read_csv(file_to_process) \n",
    "    return dataframe \n",
    "\n",
    "To extract from a JSON file, you can define the function extract_from_json()using the pandas function read_json. It requires an extra argument lines=True to enable the function to read the file as a JSON object on line to line basis as follows.\n",
    "\n",
    "def extract_from_json(file_to_process): \n",
    "    dataframe = pd.read_json(file_to_process, lines=True) \n",
    "    return dataframe \n",
    "\n",
    "To extract from an XML file, you need first to parse the data from the file using the ElementTree function. You can then extract relevant information from this data and append it to a pandas dataframe as follows.\n",
    "\n",
    "Note: Adding Data to DataFrames using pd.concat\n",
    "In this lab, we use pd.concat to append data to an existing DataFrame. This method is recommended because the append method is deprecated. pd.concat offers better efficiency and flexibility, especially when combining multiple DataFrames.\n",
    "\n",
    "Why use pd.concat:\n",
    "pd.concat is more efficient when adding rows or combining multiple DataFrames.\n",
    "It provides better control over the operation, allowing you to concatenate along rows or columns.\n",
    "It also includes the ignore_index=True argument, which resets the index to avoid duplication when combining DataFrames.\n",
    "Example:\n",
    "import pandas as pd\n",
    "# Create DataFrames\n",
    "df1 = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n",
    "df2 = pd.DataFrame({'A': [5], 'B': [6]})\n",
    "# Use concat\n",
    "result = pd.concat([df1, df2], ignore_index=True)\n",
    "print(result)\n",
    "\n",
    "Output:\n",
    "\n",
    " A B\n",
    "0 1 3\n",
    "1 2 4\n",
    "2 5 6\n",
    "\n",
    "Note: You must know the headers of the extracted data to write this function. In this data, you extract \"name\", \"height\", and \"weight\" headers for different persons.\n",
    "\n",
    "This function can be written as follows:\n",
    "\n",
    "def extract_from_xml(file_to_process): \n",
    "    dataframe = pd.DataFrame(columns=[\"name\", \"height\", \"weight\"]) \n",
    "    tree = ET.parse(file_to_process) \n",
    "    root = tree.getroot() \n",
    "    for person in root: \n",
    "        name = person.find(\"name\").text \n",
    "        height = float(person.find(\"height\").text) \n",
    "        weight = float(person.find(\"weight\").text) \n",
    "        dataframe = pd.concat([dataframe, pd.DataFrame([{\"name\":name, \"height\":height, \"weight\":weight}])], ignore_index=True) \n",
    "    return dataframe \n",
    "\n",
    "Now you need a function to identify which function to call on basis of the filetype of the data file. To call the relevant function, write a function extract, which uses the glob library to identify the filetype. This can be done as follows:\n",
    "\n",
    "def extract(): \n",
    "    extracted_data = pd.DataFrame(columns=['name','height','weight']) # create an empty data frame to hold extracted data \n",
    "     \n",
    "    # process all csv files, except the target file\n",
    "    for csvfile in glob.glob(\"*.csv\"): \n",
    "        if csvfile != target_file:  # check if the file is not the target file\n",
    "            extracted_data = pd.concat([extracted_data, pd.DataFrame(extract_from_csv(csvfile))], ignore_index=True) \n",
    "         \n",
    "    # process all json files \n",
    "    for jsonfile in glob.glob(\"*.json\"): \n",
    "        extracted_data = pd.concat([extracted_data, pd.DataFrame(extract_from_json(jsonfile))], ignore_index=True) \n",
    "     \n",
    "    # process all xml files \n",
    "    for xmlfile in glob.glob(\"*.xml\"): \n",
    "        extracted_data = pd.concat([extracted_data, pd.DataFrame(extract_from_xml(xmlfile))], ignore_index=True) \n",
    "         \n",
    "    return extracted_data \n",
    "\n",
    "After adding these functions to etl_code.py you complete the implementation of the extraction part.\n",
    "Remember to save your file using Ctrl+S."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32a7137",
   "metadata": {},
   "source": [
    "Task 2 - Transformation\n",
    "The height in the extracted data is in inches, and the weight is in pounds. However, for your application, the height is required to be in meters, and the weight is required to be in kilograms, rounded to two decimal places. Therefore, you need to write the function to perform the unit conversion for the two parameters.\n",
    "\n",
    "The name of this function will be transform(), and it will receive the extracted dataframe as the input. Since the dataframe is in the form of a dictionary with three keys, \"name\", \"height\", and \"weight\", each of them having a list of values, you can apply the transform function on the entire list at one go.\n",
    "\n",
    "The function can be written as follows:\n",
    "\n",
    "def transform(data): \n",
    "    '''Convert inches to meters and round off to two decimals \n",
    "    1 inch is 0.0254 meters '''\n",
    "    data['height'] = round(data.height * 0.0254,2) \n",
    " \n",
    "    '''Convert pounds to kilograms and round off to two decimals \n",
    "    1 pound is 0.45359237 kilograms '''\n",
    "    data['weight'] = round(data.weight * 0.45359237,2) \n",
    "    \n",
    "    return data \n",
    "\n",
    "\n",
    "The output of this function will now be a dataframe where the \"height\" and \"weight\" parameters will be modified to the required format.\n",
    "\n",
    "You can add the transform() function to the etl_code.py file, thus completing the transform operation.\n",
    "\n",
    "Remember to save your file using Ctrl+S."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6762cb",
   "metadata": {},
   "source": [
    "Task 3 - Loading and Logging\n",
    "You need to load the transformed data to a CSV file that you can use to load to a database as per requirement.\n",
    "\n",
    "To load the data, you need a function load_data() that accepts the transformed data as a dataframe and the target_file path. You need to use the to_csv attribute of the dataframe in the function as follows:\n",
    "\n",
    "def load_data(target_file, transformed_data): \n",
    "    transformed_data.to_csv(target_file) \n",
    "\n",
    "Finally, you need to implement the logging operation to record the progress of the different operations. For this operation, you need to record a message, along with its timestamp, in the log_file.\n",
    "\n",
    "To record the message, you need to implement a function log_progress() that accepts the log message as the argument. The function captures the current date and time using the datetime function from the datetime library. The use of this function requires the definition of a date-time format, and you need to convert the timestamp to a string format using the strftime attribute. The following code creates the log operation:\n",
    "\n",
    "def log_progress(message): \n",
    "    timestamp_format = '%Y-%h-%d-%H:%M:%S' # Year-Monthname-Day-Hour-Minute-Second \n",
    "    now = datetime.now() # get current timestamp \n",
    "    timestamp = now.strftime(timestamp_format) \n",
    "    with open(log_file,\"a\") as f: \n",
    "        f.write(timestamp + ',' + message + '\\n') \n",
    "After you add these functions to etl_code.py, you will complete the implementation of the loading and logging operations. With this, all the functions for Extract, Transform, and Load (ETL) are ready for testing.\n",
    "\n",
    "Remember to save your file using Ctrl+S"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bed3070",
   "metadata": {},
   "source": [
    "Testing ETL operations and log progress\n",
    "Now, test the functions you have developed so far and log your progress along the way. Insert the following lines into your code to complete the process. Note the comments on every step of the code.\n",
    "\n",
    "# Log the initialization of the ETL process \n",
    "log_progress(\"ETL Job Started\") \n",
    " \n",
    "# Log the beginning of the Extraction process \n",
    "log_progress(\"Extract phase Started\") \n",
    "extracted_data = extract() \n",
    " \n",
    "# Log the completion of the Extraction process \n",
    "log_progress(\"Extract phase Ended\") \n",
    " \n",
    "# Log the beginning of the Transformation process \n",
    "log_progress(\"Transform phase Started\") \n",
    "transformed_data = transform(extracted_data) \n",
    "print(\"Transformed Data\") \n",
    "print(transformed_data) \n",
    " \n",
    "# Log the completion of the Transformation process \n",
    "log_progress(\"Transform phase Ended\") \n",
    " \n",
    "# Log the beginning of the Loading process \n",
    "log_progress(\"Load phase Started\") \n",
    "load_data(target_file,transformed_data) \n",
    " \n",
    "# Log the completion of the Loading process \n",
    "log_progress(\"Load phase Ended\") \n",
    " \n",
    "# Log the completion of the ETL process \n",
    "log_progress(\"ETL Job Ended\") \n",
    "\n",
    "Remember to save your file using Ctrl+S."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fde288",
   "metadata": {},
   "source": [
    "Execute etl_code.py from a terminal shell using the command\n",
    "\n",
    "python3.11 etl_code.py \n",
    "Executed!\n",
    "Upon execution, you can view the output of the print command on the terminal as shown in the image below.\n",
    "![code_output](https://lh3.googleusercontent.com/u/0/d/1lw0b4Q0tcSj_DligznYu9oah6b7qXDQo)\n",
    "The contents of the log file will appear as shown in the image below.\n",
    "![log_file](https://lh3.googleusercontent.com/u/0/d/1r-qwcXbBYyKeMZQqrt78Dq6DiwyLwLfF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "827a5ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A  B\n",
      "0  1  3\n",
      "1  2  4\n",
      "2  5  6\n",
      "Transformed Data\n",
      "     name height weight\n",
      "0    alex   1.67  51.25\n",
      "1    ajay   1.82  61.91\n",
      "2   alice   1.76  69.41\n",
      "3    ravi   1.73  64.56\n",
      "4     joe   1.72  65.45\n",
      "5    alex   1.67  51.25\n",
      "6    ajay   1.82  61.91\n",
      "7   alice   1.76  69.41\n",
      "8    ravi   1.73  64.56\n",
      "9     joe   1.72  65.45\n",
      "10   alex   1.67  51.25\n",
      "11   ajay   1.82  61.91\n",
      "12  alice   1.76  69.41\n",
      "13   ravi   1.73  64.56\n",
      "14    joe   1.72  65.45\n",
      "15   jack   1.74  55.93\n",
      "16    tom   1.77  64.18\n",
      "17  tracy   1.78   61.9\n",
      "18   john   1.72  50.97\n",
      "19   jack   1.74  55.93\n",
      "20    tom   1.77  64.18\n",
      "21  tracy   1.78   61.9\n",
      "22   john   1.72  50.97\n",
      "23   jack   1.74  55.93\n",
      "24    tom   1.77  64.18\n",
      "25  tracy   1.78   61.9\n",
      "26   john   1.72  50.97\n",
      "27  simon   1.72  50.97\n",
      "28  jacob    1.7  54.73\n",
      "29  cindy   1.69  57.81\n",
      "30   ivan   1.72  51.77\n",
      "31  simon   1.72  50.97\n",
      "32  jacob    1.7  54.73\n",
      "33  cindy   1.69  57.81\n",
      "34   ivan   1.72  51.77\n",
      "35  simon   1.72  50.97\n",
      "36  jacob    1.7  54.73\n",
      "37  cindy   1.69  57.81\n",
      "38   ivan   1.72  51.77\n"
     ]
    }
   ],
   "source": [
    "import glob \n",
    "import pandas as pd \n",
    "import xml.etree.ElementTree as ET \n",
    "from datetime import datetime \n",
    "\n",
    "log_file = \"log_file.txt\" \n",
    "target_file = \"transformed_data.csv\" \n",
    "def extract_from_csv(file_to_process): \n",
    "    dataframe = pd.read_csv(file_to_process) \n",
    "    return dataframe \n",
    "\n",
    "def extract_from_json(file_to_process): \n",
    "    dataframe = pd.read_json(file_to_process, lines=True) \n",
    "    return dataframe \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create DataFrames\n",
    "df1 = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n",
    "df2 = pd.DataFrame({'A': [5], 'B': [6]})\n",
    "\n",
    "# Use concat\n",
    "result = pd.concat([df1, df2], ignore_index=True)\n",
    "print(result)\n",
    "\n",
    "def extract_from_xml(file_to_process): \n",
    "    dataframe = pd.DataFrame(columns=[\"name\", \"height\", \"weight\"]) \n",
    "    tree = ET.parse(file_to_process) \n",
    "    root = tree.getroot() \n",
    "    for person in root: \n",
    "        name = person.find(\"name\").text \n",
    "        height = float(person.find(\"height\").text) \n",
    "        weight = float(person.find(\"weight\").text) \n",
    "        dataframe = pd.concat([dataframe, pd.DataFrame([{\"name\":name, \"height\":height, \"weight\":weight}])], ignore_index=True) \n",
    "    return dataframe \n",
    "\n",
    "def extract(): \n",
    "    extracted_data = pd.DataFrame(columns=['name','height','weight']) # create an empty data frame to hold extracted data \n",
    "     \n",
    "    # process all csv files, except the target file\n",
    "    for csvfile in glob.glob(\"*.csv\"): \n",
    "        if csvfile != target_file:  # check if the file is not the target file\n",
    "            extracted_data = pd.concat([extracted_data, pd.DataFrame(extract_from_csv(csvfile))], ignore_index=True) \n",
    "         \n",
    "    # process all json files \n",
    "    for jsonfile in glob.glob(\"*.json\"): \n",
    "        extracted_data = pd.concat([extracted_data, pd.DataFrame(extract_from_json(jsonfile))], ignore_index=True) \n",
    "     \n",
    "    # process all xml files \n",
    "    for xmlfile in glob.glob(\"*.xml\"): \n",
    "        extracted_data = pd.concat([extracted_data, pd.DataFrame(extract_from_xml(xmlfile))], ignore_index=True) \n",
    "         \n",
    "    return extracted_data \n",
    "\n",
    "def transform(data): \n",
    "    '''Convert inches to meters and round off to two decimals \n",
    "    1 inch is 0.0254 meters '''\n",
    "    data['height'] = round(data.height * 0.0254,2) \n",
    " \n",
    "    '''Convert pounds to kilograms and round off to two decimals \n",
    "    1 pound is 0.45359237 kilograms '''\n",
    "    data['weight'] = round(data.weight * 0.45359237,2) \n",
    "    \n",
    "    return data \n",
    "\n",
    "def log_progress(message): \n",
    "    timestamp_format = '%Y-%h-%d-%H:%M:%S' # Year-Monthname-Day-Hour-Minute-Second \n",
    "    now = datetime.now() # get current timestamp \n",
    "    timestamp = now.strftime(timestamp_format) \n",
    "    with open(log_file,\"a\") as f: \n",
    "        f.write(timestamp + ',' + message + '\\n') \n",
    "        \n",
    "def load_data(target_file, transformed_data): \n",
    "    transformed_data.to_csv(target_file) \n",
    "# Log the initialization of the ETL process \n",
    "log_progress(\"ETL Job Started\") \n",
    " \n",
    "# Log the beginning of the Extraction process \n",
    "log_progress(\"Extract phase Started\") \n",
    "extracted_data = extract() \n",
    " \n",
    "# Log the completion of the Extraction process \n",
    "log_progress(\"Extract phase Ended\") \n",
    " \n",
    "# Log the beginning of the Transformation process \n",
    "log_progress(\"Transform phase Started\") \n",
    "transformed_data = transform(extracted_data) \n",
    "print(\"Transformed Data\") \n",
    "print(transformed_data) \n",
    " \n",
    "# Log the completion of the Transformation process \n",
    "log_progress(\"Transform phase Ended\") \n",
    " \n",
    "# Log the beginning of the Loading process \n",
    "log_progress(\"Load phase Started\") \n",
    "load_data(target_file,transformed_data) \n",
    " \n",
    "# Log the completion of the Loading process \n",
    "log_progress(\"Load phase Ended\") \n",
    " \n",
    "# Log the completion of the ETL process \n",
    "log_progress(\"ETL Job Ended\") \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
